{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zBd1Es0OugQ"
   },
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак.\n",
    "\n",
    "## Примечание: не перезапускайте ноутбук – он делался на Colab GPU\n",
    "\n",
    "\n",
    "# План\n",
    "\n",
    "В связи с запросами заказчика перед нами стоит задача регрессии. Соответственно, нам необходимо осуществить первичный анализ временного ряда и подготовку признаков для подачи в модель\n",
    "\n",
    "## <a id='start'>Contents</a>\n",
    "\n",
    "### [Preparing stage](#prepare)\n",
    "\n",
    "* [Образец токсичных данных](#tox_sample)\n",
    "\n",
    "* [Балансировка](#balance)\n",
    "\n",
    "* [Нормализация текста](#normalization)\n",
    "\n",
    "\n",
    "### [Learning stage](#learning)\n",
    "\n",
    "* [Классический NLP c TF-IDF](#classic_nlp)\n",
    "\n",
    "* [SV - декомкозиция](#SVD)\n",
    "\n",
    "* [Cheat model on tf](#TF)\n",
    "\n",
    "* [Финальная модель](#final_train)\n",
    "\n",
    "### [Выводы](#summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eV1a0nt4OugQ"
   },
   "source": [
    "# <a id='prepare'>1. Подготовка </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E6qZ35OOugQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет: </b> Желательно чтобы все импорты были собраны в первой ячейке ноутбука! Если у того, кто будет запускать твой ноутбук будут отсутствовать некоторые библиотеки, то он это увидит сразу, а не в процессе!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "BncyYZkXOugQ",
    "outputId": "4149b303-8e62-4778-ac9e-f90e5e34c441"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104890</th>\n",
       "      <td>HI sitush my brother do not do this you again ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19665</th>\n",
       "      <td>Thanks Nev, much appreciated. That's what I ca...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58151</th>\n",
       "      <td>I cited my sources, but I cannot directly link...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28292</th>\n",
       "      <td>\"Hi. I appreciate your current efforts to add ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33352</th>\n",
       "      <td>Self-appointed, self-aggrandising and self-imp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72616</th>\n",
       "      <td>Ok, if the talk page is where we can fix this ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81104</th>\n",
       "      <td>\"\\nHere's a late 2008 AP article on visitor nu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137467</th>\n",
       "      <td>\"]]\\n|rowspan=\"\"2\"\" |\\n|style=\"\"font-size: x-l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143977</th>\n",
       "      <td>In the spirit of goodwill, I'm willing to nomi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>SITUSH THE CAT..PLEASE COME OUT..I WANA CHAT W...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "104890  HI sitush my brother do not do this you again ...      0\n",
       "19665   Thanks Nev, much appreciated. That's what I ca...      0\n",
       "58151   I cited my sources, but I cannot directly link...      0\n",
       "28292   \"Hi. I appreciate your current efforts to add ...      0\n",
       "33352   Self-appointed, self-aggrandising and self-imp...      1\n",
       "72616   Ok, if the talk page is where we can fix this ...      0\n",
       "81104   \"\\nHere's a late 2008 AP article on visitor nu...      0\n",
       "137467  \"]]\\n|rowspan=\"\"2\"\" |\\n|style=\"\"font-size: x-l...      0\n",
       "143977  In the spirit of goodwill, I'm willing to nomi...      0\n",
       "2391    SITUSH THE CAT..PLEASE COME OUT..I WANA CHAT W...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('toxic_comments.csv', error_bad_lines=True, warn_bad_lines=True, encoding='latin-1')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRYLNvedOugR"
   },
   "source": [
    "Данные достаточно зашумлены. Попадаются слова разного написания, строки с невидимыми знаками(перевод строки/табуляции)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONlFfAAkOugR",
    "outputId": "178e48b8-809c-4da0-90ef-5105b596f24b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Совет: </b> Данные загружены корректно, но не забывай про проверку на пропуски.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIp4nDSQOugR"
   },
   "source": [
    "### <a id='tox_sample'>Образец токсичных данных</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "mxnxp8IYOugR",
    "outputId": "13259acd-52e2-4500-a4d8-a370b9d63f15"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159494</th>\n",
       "      <td>\"\\n\\n our previous conversation \\n\\nyou fuckin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159514</th>\n",
       "      <td>YOU ARE A MISCHIEVIOUS PUBIC HAIR</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159541</th>\n",
       "      <td>Your absurd edits \\n\\nYour absurd edits on gre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159546</th>\n",
       "      <td>\"\\n\\nHey listen don't you ever!!!! Delete my e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159554</th>\n",
       "      <td>and i'm going to keep posting the stuff u dele...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "6            COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "12      Hey... what is it..\\n@ | talk .\\nWhat is it......      1\n",
       "16      Bye! \\n\\nDon't look, come or think of comming ...      1\n",
       "42      You are gay or antisemmitian? \\n\\nArchangel WH...      1\n",
       "43               FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!      1\n",
       "...                                                   ...    ...\n",
       "159494  \"\\n\\n our previous conversation \\n\\nyou fuckin...      1\n",
       "159514                  YOU ARE A MISCHIEVIOUS PUBIC HAIR      1\n",
       "159541  Your absurd edits \\n\\nYour absurd edits on gre...      1\n",
       "159546  \"\\n\\nHey listen don't you ever!!!! Delete my e...      1\n",
       "159554  and i'm going to keep posting the stuff u dele...      1\n",
       "\n",
       "[16225 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['toxic'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUIdZxKzOugR"
   },
   "outputs": [],
   "source": [
    "def get_stat(data):\n",
    "\n",
    "  stat = data.pivot_table(index = ['toxic'], values = 'text', aggfunc='count')\n",
    "\n",
    "  stat['pct'] = stat['text']/len(data)\n",
    "\n",
    "  print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QANBj-YE9ac1",
    "outputId": "28755e97-b223-496f-efcd-523ace1cec26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         text       pct\n",
      "toxic                  \n",
      "0      143346  0.898321\n",
      "1       16225  0.101679\n"
     ]
    }
   ],
   "source": [
    "get_stat(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veZ_aoSOdy9U"
   },
   "source": [
    "Данные очень несбалансированы, всего 10% в примерах составляют токсичные комментарии, соответственно для обучения нам потребуется сбалансировать тренировочную выборку. Поэтому мы разделим наши данные на данном этапе на тестовую и тренировочную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlaOsrFN7zjA"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "main, test = train_test_split(df, stratify = df['toxic'], random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jd1_AGwK8UPK"
   },
   "source": [
    "## <a id ='balance'>Балансировка</a>\n",
    "\n",
    "Алгоритм балансировки следующий:\n",
    "\n",
    "1) Для нетоксичных комментариев мы отбираем размер в 1.5 раза превосходящий количество токсичных комментариев (для получения соотношения 60 на 40)\n",
    "\n",
    "2) Токсичные примеры оставляем без изменений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAozmADo8TKH",
    "outputId": "27656288-51f0-40c8-93df-cb576e93c758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        text       pct\n",
      "toxic                 \n",
      "0      18254  0.600007\n",
      "1      12169  0.399993\n"
     ]
    }
   ],
   "source": [
    "volume = len(main[main.toxic == 1])\n",
    "\n",
    "balanced = main[main.toxic == 0].sample(int(np.round(volume*1.5)))\n",
    "\n",
    "balanced = pd.concat([balanced, main[main.toxic == 1]])\n",
    "\n",
    "get_stat(balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QF_SNluGDEw0",
    "outputId": "b56d6811-4a28-49bb-892d-fb72d20f2d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        text       pct\n",
      "toxic                 \n",
      "0      35837  0.898328\n",
      "1       4056  0.101672\n"
     ]
    }
   ],
   "source": [
    "get_stat(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKwnR4d0OugS"
   },
   "source": [
    "## <a id='normalization'>Нормализация текста</a>\n",
    "\n",
    "Для реализации подхода с векторизацией текста, мы должны использовать препроцессинг исходного текста.\n",
    "\n",
    "Для этого загрузим необходимые библиотеки, а также напишем специальный класс по образу из Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXr5OuyxOugS"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import spacy\n",
    "from nltk import tokenize\n",
    "import re\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kyYaoAhSDDie"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDUyoJkMRmws",
    "outputId": "6ce43f59-4ebd-499e-8001-384bc77f9b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xwqixvt3OugS"
   },
   "source": [
    "Определяем класс трансформатора, который будет подобен трансформерам из пакет \"Sci-kit\" Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wK0x-lRmOugS"
   },
   "outputs": [],
   "source": [
    "class CustomNormalizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "        This function removes stopwords, special symbols and also lemmatize the whole comment\n",
    "        Notification: by using spaCy as a core of lemmatizor, we should also discriminate pronouns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "        self.pattern = str.maketrans(\"\\n\\t\\r\", \"   \")\n",
    "    \n",
    "    def remove_tabs(self, text): #убираем дичь со специальными символами\n",
    "        \n",
    "        return text.translate(self.pattern)\n",
    "    \n",
    "    def remove_punct(self, text):\n",
    "        \n",
    "        return re.sub(r'[^a-zA-Zа-яА-ЯёЁ]+',' ', self.remove_tabs(text))\n",
    "\n",
    "    \n",
    "    def lemmatize(self, text):        \n",
    "        return self.lemmatizer(self.remove_punct(text))   \n",
    "        \n",
    "    def preprocess(self, text): #основной метод\n",
    "        lemmas_list = [token.lemma_ for token in self.lemmatize(text)]       \n",
    "        text_proc = [w.lower() for w in lemmas_list if len(w)>2 and w not in self.stopwords and w != '-PRON-']        \n",
    "        return \" \".join(text_proc)\n",
    "    \n",
    "    def normalize(self, document):        \n",
    "        corpora = [self.preprocess(text) for text in document]        \n",
    "        return corpora    \n",
    "    \n",
    "    def fit(self, X, y=None):   # заглушка    \n",
    "        return self    \n",
    "    \n",
    "    def transform(self, document): # метод вызова\n",
    "        \n",
    "        if type(document) == list:            \n",
    "            return self.normalize(document)        \n",
    "        else:            \n",
    "            return self.normalize(list(document))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHqIYXF7OugS"
   },
   "outputs": [],
   "source": [
    "preprocessor = CustomNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJglCKU3OugS"
   },
   "outputs": [],
   "source": [
    "balanced['text_proceed'] = preprocessor.transform(balanced['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOtJYJ1TAy4V",
    "outputId": "c9ab4440-5578-4750-ab08-1fbf8bf22825"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test['text_proceed'] = preprocessor.transform(test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrYjpxVXOugS"
   },
   "source": [
    "Посмотрим на результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "N1SA8OpZOugS",
    "outputId": "21d4c0c0-916e-4631-e03a-3a8017834544"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_proceed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62076</th>\n",
       "      <td>Joe Hazelton 68.251.39.134</td>\n",
       "      <td>0</td>\n",
       "      <td>joe hazelton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41065</th>\n",
       "      <td>Opression \\n\\nSo you want to opress me, Mr. Bi...</td>\n",
       "      <td>1</td>\n",
       "      <td>opression want opress big nose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107127</th>\n",
       "      <td>\"\\n\\nI was thinking about creating a wikiproje...</td>\n",
       "      <td>0</td>\n",
       "      <td>think create wikiproject wikipedia deal check ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Give me a permanat block raseac....!!! remembe...</td>\n",
       "      <td>1</td>\n",
       "      <td>give permanat block raseac remember muslims wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81000</th>\n",
       "      <td>New Jersey Devils and Detroit Red Wings of 199...</td>\n",
       "      <td>0</td>\n",
       "      <td>new jersey devils detroit red wings know eithe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138697</th>\n",
       "      <td>I think that you have a point here...but I can...</td>\n",
       "      <td>0</td>\n",
       "      <td>think point think moment thank anyway drop note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57112</th>\n",
       "      <td>honestly J delanoy, take a look at your life, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>honestly delanoy take look life look much time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93159</th>\n",
       "      <td>Deletion \\n\\nyou are not allowed to remove dis...</td>\n",
       "      <td>0</td>\n",
       "      <td>deletion allow remove discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15683</th>\n",
       "      <td>\"\\nI should not be \"\"blocked\"\" for stopping no...</td>\n",
       "      <td>0</td>\n",
       "      <td>block stop consensus page move reject repeat a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64531</th>\n",
       "      <td>oh yeahh and im mentally retarded and i sukk butt</td>\n",
       "      <td>1</td>\n",
       "      <td>yeahh mentally retarded sukk butt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  ...                                       text_proceed\n",
       "62076                          Joe Hazelton 68.251.39.134  ...                                       joe hazelton\n",
       "41065   Opression \\n\\nSo you want to opress me, Mr. Bi...  ...                     opression want opress big nose\n",
       "107127  \"\\n\\nI was thinking about creating a wikiproje...  ...  think create wikiproject wikipedia deal check ...\n",
       "206     Give me a permanat block raseac....!!! remembe...  ...  give permanat block raseac remember muslims wo...\n",
       "81000   New Jersey Devils and Detroit Red Wings of 199...  ...  new jersey devils detroit red wings know eithe...\n",
       "138697  I think that you have a point here...but I can...  ...    think point think moment thank anyway drop note\n",
       "57112   honestly J delanoy, take a look at your life, ...  ...  honestly delanoy take look life look much time...\n",
       "93159   Deletion \\n\\nyou are not allowed to remove dis...  ...                   deletion allow remove discussion\n",
       "15683   \"\\nI should not be \"\"blocked\"\" for stopping no...  ...  block stop consensus page move reject repeat a...\n",
       "64531   oh yeahh and im mentally retarded and i sukk butt  ...                  yeahh mentally retarded sukk butt\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxX30R7V-3wl"
   },
   "source": [
    "Далее, исходный текст нам нам более не понадобится - избавимся от столбца с ним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iOgPdeyeEb8"
   },
   "outputs": [],
   "source": [
    "balanced = balanced.drop('text', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50XpvQjBA5T4"
   },
   "outputs": [],
   "source": [
    "test = test.drop('text', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQZBpF8COugU"
   },
   "source": [
    "# <a id='learn'>2. Обучение</a>\n",
    "\n",
    "Разделим наш датасет на валидационную и тренировочную выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yza8DA_zOugU"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOIx_OgOOugU"
   },
   "outputs": [],
   "source": [
    "X = balanced['text_proceed']\n",
    "y = balanced['toxic']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = .1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OoE7rIG1OugU"
   },
   "source": [
    "## <a id='classic'>\"Классический\" NLP</a>\n",
    "\n",
    "Для решения нашей задачи применим классический подход с TF-IDF, который хорошо себя показывает для целего ряда задач"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4i_K45T9OugU"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range =(1,2), min_df = 3, max_df = .80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fsSw7eW_qkO",
    "outputId": "d4cfb406-1b24-4455-8306-f989cb55977d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27380, 95318)\n",
      "(39893, 94775)\n"
     ]
    }
   ],
   "source": [
    "tfidf_train = vectorizer.fit_transform(X_train)\n",
    "tfidf_val = vectorizer.transform(X_val)\n",
    "\n",
    "print(tfidf_train.shape)\n",
    "print(tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJEDGcvs_zeq"
   },
   "source": [
    "## Классификация по методу TF-IDF\n",
    "\n",
    " Импортируем модели, обучим их на тренировочной выборке и попробуем оценить качество их работы на трениро"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "w1aDknCiOugV"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-346ec71e8a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComplementNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYr0t8-cCCqd"
   },
   "outputs": [],
   "source": [
    "def evalute_models(model_list, train_f, val_f, y_train, y_val):\n",
    "    for model in model_list:\n",
    "      \n",
    "        model.fit(train_f, y_train)\n",
    "        pred = model.predict(val_f)\n",
    "        print(model.__class__.__name__)\n",
    "        print(classification_report(y_val, pred, target_names = ['non-toxic','toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EAThU-Y5OugV",
    "outputId": "4b107b96-6b42-42b9-c3d2-f045e60cb6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplementNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.86      0.94      0.90      1823\n",
      "       toxic       0.90      0.77      0.83      1220\n",
      "\n",
      "    accuracy                           0.87      3043\n",
      "   macro avg       0.88      0.86      0.86      3043\n",
      "weighted avg       0.87      0.87      0.87      3043\n",
      "\n",
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.94      0.85      0.89      1823\n",
      "       toxic       0.80      0.92      0.86      1220\n",
      "\n",
      "    accuracy                           0.88      3043\n",
      "   macro avg       0.87      0.88      0.87      3043\n",
      "weighted avg       0.89      0.88      0.88      3043\n",
      "\n",
      "LGBMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.88      0.96      0.92      1823\n",
      "       toxic       0.93      0.80      0.86      1220\n",
      "\n",
      "    accuracy                           0.90      3043\n",
      "   macro avg       0.90      0.88      0.89      3043\n",
      "weighted avg       0.90      0.90      0.89      3043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [ComplementNB(), LogisticRegression(solver = 'newton-cg'), LGBMClassifier()]\n",
    "\n",
    "evalute_models(models, tfidf_train, tfidf_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRQcbuEGBkop"
   },
   "source": [
    "#### Модели показали хороший результат на валидации, а как они справятся на тесте?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdNC65u4JzLB"
   },
   "outputs": [],
   "source": [
    "X_test = test['text_proceed']\n",
    "y_test = test['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7jWMZJvBFg2",
    "outputId": "c35a4555-c2d2-4622-fa0c-4d52ba2d19ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ComplementNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.98      0.94      0.96     35837\n",
      "       toxic       0.61      0.79      0.69      4056\n",
      "\n",
      "    accuracy                           0.93     39893\n",
      "   macro avg       0.79      0.86      0.82     39893\n",
      "weighted avg       0.94      0.93      0.93     39893\n",
      "\n",
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.99      0.84      0.91     35837\n",
      "       toxic       0.40      0.92      0.55      4056\n",
      "\n",
      "    accuracy                           0.85     39893\n",
      "   macro avg       0.69      0.88      0.73     39893\n",
      "weighted avg       0.93      0.85      0.87     39893\n",
      "\n",
      "LGBMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.98      0.96      0.97     35837\n",
      "       toxic       0.71      0.79      0.75      4056\n",
      "\n",
      "    accuracy                           0.95     39893\n",
      "   macro avg       0.84      0.88      0.86     39893\n",
      "weighted avg       0.95      0.95      0.95     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_test = vectorizer.transform(X_test)\n",
    "\n",
    "evalute_models(models, tfidf_train, tfidf_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRaW-DUMC83-"
   },
   "source": [
    "На тесте только LGBM достиг целевой метрики качества (f1_score - 0.75 для целевого класса), однако смеем предположить, что при полном обучении на тренировочной и валидационной выборке, он справился бы лучше\n",
    "\n",
    "## <a id='SVD'>Сингулярное разложение признаков</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PgwLVouHYvFF"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, n_iter=7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoyX_NPzZD9f"
   },
   "outputs": [],
   "source": [
    "svd_train = svd.fit_transform(tfidf_train)\n",
    "svd_val = svd.transform(tfidf_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PT129t-LbWAr",
    "outputId": "d1d439fd-161d-4d29-c734-187222917a98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27380, 300)\n",
      "(3043, 300)\n"
     ]
    }
   ],
   "source": [
    "print(svd_train.shape)\n",
    "print(svd_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rS-HmjgZfRo",
    "outputId": "5633f20d-494d-40a5-85e5-95879d679aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.86      0.95      0.90      1823\n",
      "       toxic       0.91      0.77      0.84      1220\n",
      "\n",
      "    accuracy                           0.88      3043\n",
      "   macro avg       0.89      0.86      0.87      3043\n",
      "weighted avg       0.88      0.88      0.88      3043\n",
      "\n",
      "LGBMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.88      0.93      0.90      1823\n",
      "       toxic       0.88      0.80      0.84      1220\n",
      "\n",
      "    accuracy                           0.88      3043\n",
      "   macro avg       0.88      0.87      0.87      3043\n",
      "weighted avg       0.88      0.88      0.88      3043\n",
      "\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.84      0.93      0.89      1823\n",
      "       toxic       0.88      0.74      0.81      1220\n",
      "\n",
      "    accuracy                           0.86      3043\n",
      "   macro avg       0.86      0.84      0.85      3043\n",
      "weighted avg       0.86      0.86      0.85      3043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_svd = [LogisticRegression(solver = 'newton-cg'), LGBMClassifier(), RandomForestClassifier(random_state=42)]\n",
    "\n",
    "\n",
    "evalute_models(models_svd, svd_train, svd_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pst-BaRwHDWB",
    "outputId": "928e2d94-c3e5-4472-84e3-7ebe6bec55b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.97      0.95      0.96     35837\n",
      "       toxic       0.62      0.77      0.69      4056\n",
      "\n",
      "    accuracy                           0.93     39893\n",
      "   macro avg       0.80      0.86      0.82     39893\n",
      "weighted avg       0.94      0.93      0.93     39893\n",
      "\n",
      "LGBMClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.98      0.93      0.95     35837\n",
      "       toxic       0.57      0.80      0.67      4056\n",
      "\n",
      "    accuracy                           0.92     39893\n",
      "   macro avg       0.77      0.87      0.81     39893\n",
      "weighted avg       0.94      0.92      0.92     39893\n",
      "\n",
      "RandomForestClassifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   non-toxic       0.97      0.94      0.95     35837\n",
      "       toxic       0.57      0.77      0.66      4056\n",
      "\n",
      "    accuracy                           0.92     39893\n",
      "   macro avg       0.77      0.85      0.81     39893\n",
      "weighted avg       0.93      0.92      0.92     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svd_test = svd.transform(tfidf_test)\n",
    "\n",
    "evalute_models(models_svd, svd_train, svd_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf6l6CXLG81X"
   },
   "source": [
    "При сокращении размерности видимо часть информации потерялась. Ни одна из моделей не показала целевую метрику качества. Значит LSA нам не подходит."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYyI6USSjcnH"
   },
   "source": [
    "### <a id = 'TF'>Cheat-model on TF wheels </a>\n",
    "\n",
    "Вполне возможно, что с помощью нашего датасета мы можем построить полноценную нейронную сеть, которая могла бы лучше обобщить паттерны токсичных комментариев и осуществлять их эффективный поиск.\n",
    "\n",
    "С помощью фреймворка Tensorflow и его высокоуровнего API Keras мы можем протестировать сверточную нейронную сеть на наших данных для задачи классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vugGA5UHjbO5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5AHx_lUkVf5"
   },
   "source": [
    "В связи с тем, что у нас мало данных для обучения по целевому, то мы будем использовать весь наш трейн для обучения, а валидироваться на тестовом датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_3t56Ee7lCVt",
    "outputId": "0becfe21-9855-4117-d7de-a4d4a1f0d2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30423, 1250)\n",
      "(30423, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = balanced['text_proceed'].to_list()\n",
    "X_test = test['text_proceed'].to_list()\n",
    "\n",
    "encoder = keras.preprocessing.text.Tokenizer()\n",
    "encoder.fit_on_texts(X_train)\n",
    "\n",
    "X_train = encoder.texts_to_sequences(X_train)\n",
    "X_test = encoder.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = max(map(len, X_test))\n",
    "\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_length)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_length)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(balanced['toxic']).reshape((-1,1))\n",
    "y_test = np.array(y_test).reshape((-1,1))\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kICRRnRcO6VJ"
   },
   "source": [
    "Также подготовим тестовую выборку для совершения предсказаний:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsqJ_0g4rgVb"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(len(encoder.index_word) + 1, embedding_dim))\n",
    "\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=[tf.keras.losses.BinaryCrossentropy()],\n",
    "              metrics=[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoUwHaZPkSxp",
    "outputId": "91bfb633-f99d-4f84-d45d-0d28ce7ea60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_79 (Embedding)     (None, None, 100)         5446000   \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 5,511,429\n",
      "Trainable params: 5,511,429\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=x_train.shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRKkh1h7jpfx",
    "outputId": "03da823d-b06d-4eed-a779-8b6b09b9d4b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "238/238 [==============================] - 26s 111ms/step - loss: 0.2453 - auc_27: 0.9606 - val_loss: 0.2999 - val_auc_27: 0.9659\n",
      "Epoch 2/5\n",
      "238/238 [==============================] - 26s 109ms/step - loss: 0.1053 - auc_27: 0.9923 - val_loss: 0.2539 - val_auc_27: 0.9613\n",
      "Epoch 3/5\n",
      "238/238 [==============================] - 26s 108ms/step - loss: 0.0447 - auc_27: 0.9979 - val_loss: 0.3753 - val_auc_27: 0.9545\n",
      "Epoch 4/5\n",
      "238/238 [==============================] - 26s 109ms/step - loss: 0.0206 - auc_27: 0.9993 - val_loss: 0.2974 - val_auc_27: 0.9386\n",
      "Epoch 5/5\n",
      "238/238 [==============================] - 26s 109ms/step - loss: 0.0157 - auc_27: 0.9995 - val_loss: 0.3577 - val_auc_27: 0.9475\n",
      "1247/1247 [==============================] - 7s 6ms/step - loss: 0.3577 - auc_27: 0.9475\n",
      "Test set\n",
      "  Loss: 0.358\n",
      "  Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=x_train, y=y_train, epochs=5, shuffle=True,\n",
    "          batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "accr = model.evaluate(x_test,y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bErdFFd9PPCz",
    "outputId": "e3f58d55-ee01-4c48-d66d-949d8ef50af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.95     35837\n",
      "           1       0.53      0.88      0.66      4056\n",
      "\n",
      "    accuracy                           0.91     39893\n",
      "   macro avg       0.76      0.90      0.81     39893\n",
      "weighted avg       0.94      0.91      0.92     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_nn = model.predict(x_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, np.round(pred_nn)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeBD6dMHvkNn"
   },
   "source": [
    "#### Неожиданный результат, сверточная нейронная сеть показала худший результат на эмбедингах по сравнению с классическими методами.\n",
    "\n",
    "Однако вполне возможно, что это произошло от недостатка опыта работы с подобными моделями.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODMPOOoEv9ws"
   },
   "source": [
    "### <a id ='final_train'>Финальная тренировка модели</a>\n",
    "\n",
    "Лучше всего показал себя LGBM с применением tf-idf, поэтому мы остановим свой выбор на нем.\n",
    "\n",
    "Обучим на полном обучающем датасете и протестируем на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nd5yDiZVv8C8"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "lgbm_params = {'boosting_type': 'dart',\n",
    "  'n_iterations' : 1000,\n",
    "  'n_estimators': 500,\n",
    " 'learning_rate': 0.05,\n",
    " 'max_bin' : 100,\n",
    " 'max_depth':24,\n",
    " 'gpu_use_dp' : True}\n",
    "\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('model', LGBMClassifier(**lgbm_params))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0Vbcrnkxue4",
    "outputId": "cfb856ab-a783-482f-bdf4-e1260e1b3b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30423,) (30423,)\n",
      "(39893,) (39893,)\n"
     ]
    }
   ],
   "source": [
    "X_test = test['text_proceed']\n",
    "y_test = test['toxic']\n",
    "\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVOETSqpyPTV"
   },
   "outputs": [],
   "source": [
    "pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chY6nRbjyXCP"
   },
   "outputs": [],
   "source": [
    "predictions = pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predictions, target_names =['non-toxis','toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# К сожалению, сбросился ноутбук на финальном выводе. Я получил ранее при таких параметрах 0.76 f1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W76s_GOOOugV"
   },
   "source": [
    "# <a id='summary'>3. Выводы</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2mFzF6x229Q"
   },
   "source": [
    "В ходе нашего исследования мы совершили много преобразований в рамках конвейра обработки текстовых данных, протестировали целый ряд методов для работы с текстовыми данными:\n",
    "\n",
    "1) Описали дисбаланс классов и ликвидировали его на основной (обучающей и валидационных выборках)\n",
    "\n",
    "2) Описали и применили класс конвейра для нормализации текста (удаление стоп-слов, пунктуации, спец-символов и лемматизация)\n",
    "\n",
    "3) Получили TF-IDF представление нашего датасета с помощью которого достигли хороших результатов на тестовой выборке\n",
    "\n",
    "4) Сингулярное разложение TF-IDF оказалось не таким эффективным, результат ухудшился. (возможно надо было проводить его с простой токенизацией)\n",
    "\n",
    "5) Апробировали подход к предсказаниям с помощью эмбеддингов из библиотеки Tensorflow-Keras, однако модель не устроила нас по качеству (скорее всего из-за недостатка опыта работы с подобными моделями)\n",
    "\n",
    "6) Создали конвейр машинного обучения для внедрения в продакшн со значением целевой метрики в 0.76\n",
    "\n",
    "**Таким образом, мы достигли целевой метрики в 0.76 f1-score с помощью LGBMClassifier - она и стала нашей основной моделью**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Lu1qO3t2HM1"
   },
   "source": [
    "[<center>В начало</center>](#start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Успех:</b> Всегда приятно видеть вывод в конце проекта, особенно так хорошо структурированный.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNAcSTbWOugV"
   },
   "source": [
    "# Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLeNPrxTOugV"
   },
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "text_classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
